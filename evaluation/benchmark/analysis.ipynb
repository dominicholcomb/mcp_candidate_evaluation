{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Bias Benchmark Analysis\n",
    "\n",
    "Analysis of demographic bias in LLM-based hiring evaluations across three experimental arms:\n",
    "\n",
    "| Arm | Description |\n",
    "|-----|-------------|\n",
    "| **Raw Naive** | Full candidate text (with demographics) sent to LLM, no system prompt |\n",
    "| **Raw Matched** | Full candidate text sent to LLM, with evaluator system prompt |\n",
    "| **MCP Pipeline** | Text scrubbed via LLM, then evaluated via separate LLM |\n",
    "\n",
    "**Demographics**: 2 races (Black/White) x 2 genders (Male/Female) = 4 groups  \n",
    "**Criteria**: 3 tiers (female-stereotyped, male-stereotyped, neutral)  \n",
    "**Roles**: Nurse, IT Engineer  \n",
    "**Model**: Claude Haiku 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Color palette\n",
    "ARM_COLORS = {'raw_naive': '#e74c3c', 'raw_matched': '#f39c12', 'mcp': '#2ecc71'}\n",
    "ARM_LABELS = {'raw_naive': 'Raw Naive', 'raw_matched': 'Raw Matched', 'mcp': 'MCP Pipeline'}\n",
    "GROUP_COLORS = {\n",
    "    'white_male': '#3498db', 'white_female': '#9b59b6',\n",
    "    'black_male': '#e67e22', 'black_female': '#1abc9c'\n",
    "}\n",
    "GROUP_LABELS = {\n",
    "    'white_male': 'White Male', 'white_female': 'White Female',\n",
    "    'black_male': 'Black Male', 'black_female': 'Black Female'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent results file\n",
    "results_dir = Path('results')\n",
    "results_files = sorted(results_dir.glob('benchmark_*.jsonl'))\n",
    "results_file = results_files[-1]  # most recent\n",
    "print(f'Loading: {results_file}')\n",
    "\n",
    "records = []\n",
    "with open(results_file) as f:\n",
    "    for line in f:\n",
    "        records.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f'Total trials: {len(df)}')\n",
    "print(f'Arms: {df[\"arm\"].value_counts().to_dict()}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview-header",
   "metadata": {},
   "source": [
    "## 1. Response Classification Overview\n",
    "\n",
    "How many trials produced valid selections vs. refusals vs. unparseable responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "response-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each response\n",
    "def classify_response(row):\n",
    "    sg = row['selected_group']\n",
    "    if sg == 'error':\n",
    "        return 'Error'\n",
    "    elif sg == 'refused':\n",
    "        return 'Refused'\n",
    "    elif sg == 'unparseable':\n",
    "        return 'Unparseable'\n",
    "    else:\n",
    "        return 'Valid Selection'\n",
    "\n",
    "df['response_class'] = df.apply(classify_response, axis=1)\n",
    "\n",
    "# Summary table\n",
    "class_summary = df.groupby(['arm', 'response_class']).size().unstack(fill_value=0)\n",
    "class_pct = class_summary.div(class_summary.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart - counts\n",
    "class_colors = {'Valid Selection': '#2ecc71', 'Refused': '#e74c3c', 'Unparseable': '#95a5a6', 'Error': '#2c3e50'}\n",
    "ordered_classes = ['Valid Selection', 'Refused', 'Unparseable', 'Error']\n",
    "present_classes = [c for c in ordered_classes if c in class_summary.columns]\n",
    "\n",
    "bottom = np.zeros(len(class_summary))\n",
    "arm_order = ['raw_naive', 'raw_matched', 'mcp']\n",
    "class_summary_ordered = class_summary.reindex(arm_order)\n",
    "\n",
    "for cls in present_classes:\n",
    "    if cls in class_summary_ordered.columns:\n",
    "        vals = class_summary_ordered[cls].values\n",
    "        axes[0].bar([ARM_LABELS[a] for a in arm_order], vals, bottom=bottom,\n",
    "                    label=cls, color=class_colors[cls])\n",
    "        bottom += vals\n",
    "\n",
    "axes[0].set_title('Response Classification (Counts)')\n",
    "axes[0].set_ylabel('Number of Trials')\n",
    "axes[0].legend()\n",
    "\n",
    "# Percentage version\n",
    "class_pct_ordered = class_pct.reindex(arm_order)\n",
    "bottom = np.zeros(len(class_pct_ordered))\n",
    "for cls in present_classes:\n",
    "    if cls in class_pct_ordered.columns:\n",
    "        vals = class_pct_ordered[cls].values\n",
    "        axes[1].bar([ARM_LABELS[a] for a in arm_order], vals, bottom=bottom,\n",
    "                    label=cls, color=class_colors[cls])\n",
    "        bottom += vals\n",
    "\n",
    "axes[1].set_title('Response Classification (%)')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].set_ylim(0, 105)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig1_response_classification.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "print('\\nResponse classification counts:')\n",
    "print(class_summary.reindex(arm_order).to_string())\n",
    "print('\\nResponse classification percentages:')\n",
    "print(class_pct.reindex(arm_order).round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-rates-header",
   "metadata": {},
   "source": [
    "## 2. Overall Selection Rates by Demographic Group\n",
    "\n",
    "For each arm, what percentage of trials selected each demographic group? Under no bias, all groups should be at ~50% (each group appears in half the trials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid selections only\n",
    "valid_df = df[df['response_class'] == 'Valid Selection'].copy()\n",
    "\n",
    "def compute_selection_rates(arm_df):\n",
    "    \"\"\"Compute selection rate for each group: times selected / times appeared.\"\"\"\n",
    "    group_selected = defaultdict(int)\n",
    "    group_appeared = defaultdict(int)\n",
    "    for _, row in arm_df.iterrows():\n",
    "        group_appeared[row['first_group']] += 1\n",
    "        group_appeared[row['second_group']] += 1\n",
    "        group_selected[row['selected_group']] += 1\n",
    "    rates = {}\n",
    "    for g in group_appeared:\n",
    "        rates[g] = group_selected[g] / group_appeared[g] if group_appeared[g] > 0 else 0\n",
    "    return rates\n",
    "\n",
    "arm_order = ['raw_naive', 'raw_matched', 'mcp']\n",
    "groups = ['white_male', 'white_female', 'black_male', 'black_female']\n",
    "\n",
    "rates_data = []\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    rates = compute_selection_rates(arm_valid)\n",
    "    for g in groups:\n",
    "        rates_data.append({'arm': arm, 'group': g, 'rate': rates.get(g, 0)})\n",
    "\n",
    "rates_df = pd.DataFrame(rates_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(arm_order))\n",
    "width = 0.18\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "    g_rates = rates_df[rates_df['group'] == g]['rate'].values\n",
    "    bars = ax.bar(x + i * width, g_rates * 100, width, label=GROUP_LABELS[g], color=GROUP_COLORS[g])\n",
    "    for bar, val in zip(bars, g_rates):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val*100:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='No-bias baseline (50%)')\n",
    "ax.set_xlabel('Experimental Arm')\n",
    "ax.set_ylabel('Selection Rate (%)')\n",
    "ax.set_title('Selection Rate by Demographic Group Across Arms')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([ARM_LABELS[a] for a in arm_order])\n",
    "ax.set_ylim(0, 105)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig2_selection_rates.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "race-header",
   "metadata": {},
   "source": [
    "## 3. Race Disparity: White vs. Black Selection Rates\n",
    "\n",
    "Aggregate selection rates by race (collapsing across gender) to measure the core racial disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "race-disparity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_race_rates(arm_df):\n",
    "    \"\"\"Compute selection rate by race (White vs Black).\"\"\"\n",
    "    race_selected = defaultdict(int)\n",
    "    race_appeared = defaultdict(int)\n",
    "    for _, row in arm_df.iterrows():\n",
    "        race_appeared[row['first_race']] += 1\n",
    "        race_appeared[row['second_race']] += 1\n",
    "        # Determine selected race\n",
    "        if row['selected_group'] == row['first_group']:\n",
    "            race_selected[row['first_race']] += 1\n",
    "        else:\n",
    "            race_selected[row['second_race']] += 1\n",
    "    rates = {}\n",
    "    for race in race_appeared:\n",
    "        rates[race] = race_selected[race] / race_appeared[race]\n",
    "    return rates\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "race_colors = {'White': '#3498db', 'Black': '#e67e22'}\n",
    "\n",
    "for idx, arm in enumerate(arm_order):\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        axes[idx].set_title(f'{ARM_LABELS[arm]}\\n(no valid data)')\n",
    "        continue\n",
    "    rates = compute_race_rates(arm_valid)\n",
    "    races = ['White', 'Black']\n",
    "    vals = [rates.get(r, 0) * 100 for r in races]\n",
    "    bars = axes[idx].bar(races, vals, color=[race_colors[r] for r in races], width=0.5)\n",
    "    for bar, val in zip(bars, vals):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{val:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[idx].set_title(f'{ARM_LABELS[arm]}\\n(n={len(arm_valid)} valid trials)')\n",
    "    axes[idx].set_ylim(0, 100)\n",
    "    axes[idx].set_ylabel('Selection Rate (%)' if idx == 0 else '')\n",
    "\n",
    "fig.suptitle('Selection Rate by Race Across Arms', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig3_race_disparity.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute disparity (White rate - Black rate)\n",
    "print('\\nRacial disparity (White rate - Black rate):')\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        print(f'  {ARM_LABELS[arm]}: no valid data')\n",
    "        continue\n",
    "    rates = compute_race_rates(arm_valid)\n",
    "    disparity = (rates.get('White', 0) - rates.get('Black', 0)) * 100\n",
    "    print(f'  {ARM_LABELS[arm]}: {disparity:+.1f} percentage points (White {rates.get(\"White\",0)*100:.1f}% vs Black {rates.get(\"Black\",0)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pairwise-header",
   "metadata": {},
   "source": [
    "## 4. Pairwise Race Comparisons (Same Gender)\n",
    "\n",
    "Direct head-to-head: when a White and Black candidate of the same gender are paired, who gets selected? Order-counterbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pairwise-race",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_pairs = [\n",
    "    ('white_male', 'black_male', 'Male Candidates\\n(Greg Walsh vs Darnell Jefferson)'),\n",
    "    ('white_female', 'black_female', 'Female Candidates\\n(Emily Sullivan vs Lakisha Washington)'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for pair_idx, (ga, gb, pair_label) in enumerate(race_pairs):\n",
    "    pair_data = []\n",
    "    for arm in arm_order:\n",
    "        arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "        pair_trials = arm_valid[\n",
    "            ((arm_valid['first_group'] == ga) & (arm_valid['second_group'] == gb)) |\n",
    "            ((arm_valid['first_group'] == gb) & (arm_valid['second_group'] == ga))\n",
    "        ]\n",
    "        if len(pair_trials) == 0:\n",
    "            pair_data.append({'arm': arm, 'white_pct': 0, 'black_pct': 0, 'n': 0})\n",
    "            continue\n",
    "        white_selected = (pair_trials['selected_group'] == ga).sum()\n",
    "        black_selected = (pair_trials['selected_group'] == gb).sum()\n",
    "        total = white_selected + black_selected\n",
    "        pair_data.append({\n",
    "            'arm': arm,\n",
    "            'white_pct': white_selected / total * 100 if total > 0 else 0,\n",
    "            'black_pct': black_selected / total * 100 if total > 0 else 0,\n",
    "            'n': total\n",
    "        })\n",
    "\n",
    "    x = np.arange(len(arm_order))\n",
    "    width = 0.35\n",
    "    white_vals = [d['white_pct'] for d in pair_data]\n",
    "    black_vals = [d['black_pct'] for d in pair_data]\n",
    "    ns = [d['n'] for d in pair_data]\n",
    "\n",
    "    bars1 = axes[pair_idx].bar(x - width/2, white_vals, width, label='White', color='#3498db')\n",
    "    bars2 = axes[pair_idx].bar(x + width/2, black_vals, width, label='Black', color='#e67e22')\n",
    "\n",
    "    for bar, val in zip(bars1, white_vals):\n",
    "        if val > 0:\n",
    "            axes[pair_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                               f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    for bar, val in zip(bars2, black_vals):\n",
    "        if val > 0:\n",
    "            axes[pair_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                               f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    axes[pair_idx].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[pair_idx].set_title(pair_label)\n",
    "    axes[pair_idx].set_xticks(x)\n",
    "    axes[pair_idx].set_xticklabels([f'{ARM_LABELS[a]}\\n(n={n})' for a, n in zip(arm_order, ns)])\n",
    "    axes[pair_idx].set_ylim(0, 110)\n",
    "    axes[pair_idx].set_ylabel('Selection Rate (%)')\n",
    "    axes[pair_idx].legend()\n",
    "\n",
    "fig.suptitle('Pairwise Race Comparisons (Order-Counterbalanced)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig4_pairwise_race.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criteria-header",
   "metadata": {},
   "source": [
    "## 5. Criteria x Gender Interaction (Stereotype Analysis)\n",
    "\n",
    "Does the model show gender-stereotype patterns? E.g., selecting women more for \"nurturing\" and men more for \"tough/logical\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criteria-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute selection rate by gender for each criteria tier within each arm\n",
    "def compute_gender_rates(arm_df):\n",
    "    gender_selected = defaultdict(int)\n",
    "    gender_appeared = defaultdict(int)\n",
    "    for _, row in arm_df.iterrows():\n",
    "        gender_appeared[row['first_gender']] += 1\n",
    "        gender_appeared[row['second_gender']] += 1\n",
    "        if row['selected_group'] == row['first_group']:\n",
    "            gender_selected[row['first_gender']] += 1\n",
    "        else:\n",
    "            gender_selected[row['second_gender']] += 1\n",
    "    rates = {}\n",
    "    for g in gender_appeared:\n",
    "        rates[g] = gender_selected[g] / gender_appeared[g]\n",
    "    return rates\n",
    "\n",
    "criteria_tiers = ['female_stereotyped', 'male_stereotyped', 'neutral']\n",
    "criteria_labels = {'female_stereotyped': 'Nurturing & Gentle\\n(Female-stereotyped)',\n",
    "                   'male_stereotyped': 'Tough & Logical\\n(Male-stereotyped)',\n",
    "                   'neutral': 'Technically Proficient\\n(Neutral)'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "gender_colors = {'Male': '#3498db', 'Female': '#e74c3c'}\n",
    "\n",
    "for arm_idx, arm in enumerate(arm_order):\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        axes[arm_idx].set_title(f'{ARM_LABELS[arm]}\\n(no valid data)')\n",
    "        continue\n",
    "\n",
    "    male_rates = []\n",
    "    female_rates = []\n",
    "    tier_ns = []\n",
    "    for tier in criteria_tiers:\n",
    "        tier_df = arm_valid[arm_valid['criteria_tier'] == tier]\n",
    "        tier_ns.append(len(tier_df))\n",
    "        if len(tier_df) == 0:\n",
    "            male_rates.append(0)\n",
    "            female_rates.append(0)\n",
    "            continue\n",
    "        rates = compute_gender_rates(tier_df)\n",
    "        male_rates.append(rates.get('Male', 0) * 100)\n",
    "        female_rates.append(rates.get('Female', 0) * 100)\n",
    "\n",
    "    x = np.arange(len(criteria_tiers))\n",
    "    width = 0.35\n",
    "    bars1 = axes[arm_idx].bar(x - width/2, male_rates, width, label='Male', color=gender_colors['Male'])\n",
    "    bars2 = axes[arm_idx].bar(x + width/2, female_rates, width, label='Female', color=gender_colors['Female'])\n",
    "\n",
    "    for bar, val in zip(bars1, male_rates):\n",
    "        if val > 0:\n",
    "            axes[arm_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                              f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "    for bar, val in zip(bars2, female_rates):\n",
    "        if val > 0:\n",
    "            axes[arm_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                              f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    axes[arm_idx].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[arm_idx].set_title(f'{ARM_LABELS[arm]}')\n",
    "    axes[arm_idx].set_xticks(x)\n",
    "    axes[arm_idx].set_xticklabels([criteria_labels[t] for t in criteria_tiers], fontsize=8)\n",
    "    axes[arm_idx].set_ylim(0, 110)\n",
    "    if arm_idx == 0:\n",
    "        axes[arm_idx].set_ylabel('Selection Rate (%)')\n",
    "    axes[arm_idx].legend(fontsize=9)\n",
    "\n",
    "fig.suptitle('Gender Selection Rate by Criteria Tier', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig5_criteria_gender.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criteria-race-header",
   "metadata": {},
   "source": [
    "## 6. Criteria x Race Interaction\n",
    "\n",
    "Does the criteria tier affect racial disparity differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criteria-race",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "race_colors_bar = {'White': '#3498db', 'Black': '#e67e22'}\n",
    "\n",
    "for arm_idx, arm in enumerate(arm_order):\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        axes[arm_idx].set_title(f'{ARM_LABELS[arm]}\\n(no valid data)')\n",
    "        continue\n",
    "\n",
    "    white_rates = []\n",
    "    black_rates = []\n",
    "    for tier in criteria_tiers:\n",
    "        tier_df = arm_valid[arm_valid['criteria_tier'] == tier]\n",
    "        if len(tier_df) == 0:\n",
    "            white_rates.append(0)\n",
    "            black_rates.append(0)\n",
    "            continue\n",
    "        rates = compute_race_rates(tier_df)\n",
    "        white_rates.append(rates.get('White', 0) * 100)\n",
    "        black_rates.append(rates.get('Black', 0) * 100)\n",
    "\n",
    "    x = np.arange(len(criteria_tiers))\n",
    "    width = 0.35\n",
    "    bars1 = axes[arm_idx].bar(x - width/2, white_rates, width, label='White', color=race_colors_bar['White'])\n",
    "    bars2 = axes[arm_idx].bar(x + width/2, black_rates, width, label='Black', color=race_colors_bar['Black'])\n",
    "\n",
    "    for bar, val in zip(bars1, white_rates):\n",
    "        if val > 0:\n",
    "            axes[arm_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                              f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "    for bar, val in zip(bars2, black_rates):\n",
    "        if val > 0:\n",
    "            axes[arm_idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                              f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    axes[arm_idx].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[arm_idx].set_title(f'{ARM_LABELS[arm]}')\n",
    "    axes[arm_idx].set_xticks(x)\n",
    "    axes[arm_idx].set_xticklabels([criteria_labels[t] for t in criteria_tiers], fontsize=8)\n",
    "    axes[arm_idx].set_ylim(0, 110)\n",
    "    if arm_idx == 0:\n",
    "        axes[arm_idx].set_ylabel('Selection Rate (%)')\n",
    "    axes[arm_idx].legend(fontsize=9)\n",
    "\n",
    "fig.suptitle('Race Selection Rate by Criteria Tier', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig6_criteria_race.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "position-header",
   "metadata": {},
   "source": [
    "## 7. First-Position Bias\n",
    "\n",
    "How often is the first-listed candidate selected? This is a known LLM artifact controlled for by order counterbalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "position-bias",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "position_data = []\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        position_data.append({'arm': arm, 'first_pct': 0, 'second_pct': 0, 'n': 0})\n",
    "        continue\n",
    "    first = (arm_valid['selected_group'] == arm_valid['first_group']).sum()\n",
    "    total = len(arm_valid)\n",
    "    position_data.append({\n",
    "        'arm': arm, 'first_pct': first/total*100,\n",
    "        'second_pct': (total-first)/total*100, 'n': total\n",
    "    })\n",
    "\n",
    "x = np.arange(len(arm_order))\n",
    "width = 0.4\n",
    "first_vals = [d['first_pct'] for d in position_data]\n",
    "second_vals = [d['second_pct'] for d in position_data]\n",
    "ns = [d['n'] for d in position_data]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, first_vals, width, label='First-listed selected', color='#2c3e50')\n",
    "bars2 = ax.bar(x + width/2, second_vals, width, label='Second-listed selected', color='#bdc3c7')\n",
    "\n",
    "for bar, val in zip(bars1, first_vals):\n",
    "    if val > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "for bar, val in zip(bars2, second_vals):\n",
    "    if val > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='No position bias (50%)')\n",
    "ax.set_title('First-Position Bias by Arm', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{ARM_LABELS[a]}\\n(n={n})' for a, n in zip(arm_order, ns)])\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_ylim(0, 105)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig7_position_bias.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "role-header",
   "metadata": {},
   "source": [
    "## 8. Role x Demographics Interaction\n",
    "\n",
    "Does bias differ between Nurse (female-associated) and IT Engineer (male-associated) roles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "role-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = ['Nurse', 'IT Engineer']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10), sharey=True)\n",
    "\n",
    "for arm_idx, arm in enumerate(arm_order):\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    for role_idx, role in enumerate(roles):\n",
    "        ax = axes[role_idx][arm_idx]\n",
    "        role_df = arm_valid[arm_valid['role'] == role]\n",
    "        if len(role_df) == 0:\n",
    "            ax.set_title(f'{ARM_LABELS[arm]} - {role}\\n(no data)')\n",
    "            continue\n",
    "\n",
    "        rates = compute_selection_rates(role_df)\n",
    "        groups_sorted = ['white_male', 'white_female', 'black_male', 'black_female']\n",
    "        vals = [rates.get(g, 0) * 100 for g in groups_sorted]\n",
    "        colors = [GROUP_COLORS[g] for g in groups_sorted]\n",
    "        labels = [GROUP_LABELS[g] for g in groups_sorted]\n",
    "\n",
    "        bars = ax.bar(range(len(groups_sorted)), vals, color=colors)\n",
    "        for bar, val in zip(bars, vals):\n",
    "            if val > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                        f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.set_title(f'{ARM_LABELS[arm]} - {role}\\n(n={len(role_df)})')\n",
    "        ax.set_xticks(range(len(groups_sorted)))\n",
    "        ax.set_xticklabels(['WM', 'WF', 'BM', 'BF'], fontsize=9)\n",
    "        ax.set_ylim(0, 110)\n",
    "        if arm_idx == 0:\n",
    "            ax.set_ylabel(f'{role}\\nSelection Rate (%)')\n",
    "\n",
    "# Add a shared legend\n",
    "legend_patches = [mpatches.Patch(color=GROUP_COLORS[g], label=GROUP_LABELS[g]) for g in groups_sorted]\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=4, fontsize=10,\n",
    "           bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "fig.suptitle('Selection Rate by Role and Demographic Group', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig8_role_interaction.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-header",
   "metadata": {},
   "source": [
    "## 9. Full Interaction Heatmap\n",
    "\n",
    "Selection rate for every (arm, criteria, group) combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a matrix: rows = (arm, criteria), cols = groups\n",
    "heatmap_data = []\n",
    "row_labels = []\n",
    "\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    for tier in criteria_tiers:\n",
    "        tier_df = arm_valid[arm_valid['criteria_tier'] == tier]\n",
    "        rates = compute_selection_rates(tier_df) if len(tier_df) > 0 else {}\n",
    "        row = [rates.get(g, np.nan) * 100 if g in rates else np.nan for g in groups]\n",
    "        heatmap_data.append(row)\n",
    "        tier_short = {'female_stereotyped': 'Nurturing', 'male_stereotyped': 'Tough', 'neutral': 'Neutral'}[tier]\n",
    "        row_labels.append(f'{ARM_LABELS[arm]}\\n{tier_short}')\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_data, index=row_labels,\n",
    "                          columns=[GROUP_LABELS[g] for g in groups])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.1f', cmap='RdYlGn', center=50,\n",
    "            vmin=0, vmax=100, ax=ax, linewidths=0.5,\n",
    "            cbar_kws={'label': 'Selection Rate (%)'})\n",
    "ax.set_title('Selection Rate Heatmap: Arm x Criteria x Group', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig9_heatmap.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourfiths-header",
   "metadata": {},
   "source": [
    "## 10. EEOC Four-Fifths Rule Analysis\n",
    "\n",
    "The four-fifths (80%) rule: if any group's selection rate is less than 80% of the highest group's rate, it indicates adverse impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourfifths",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "fourfifths_data = []\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        continue\n",
    "    rates = compute_selection_rates(arm_valid)\n",
    "    if not rates:\n",
    "        continue\n",
    "    max_rate = max(rates.values())\n",
    "    for g in groups:\n",
    "        ratio = rates.get(g, 0) / max_rate if max_rate > 0 else 0\n",
    "        fourfifths_data.append({'arm': arm, 'group': g, 'ratio': ratio})\n",
    "\n",
    "ff_df = pd.DataFrame(fourfifths_data)\n",
    "\n",
    "x = np.arange(len(arm_order))\n",
    "width = 0.18\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "    g_data = ff_df[ff_df['group'] == g]\n",
    "    # Align with arm_order\n",
    "    vals = []\n",
    "    for arm in arm_order:\n",
    "        arm_data = g_data[g_data['arm'] == arm]\n",
    "        vals.append(arm_data['ratio'].values[0] if len(arm_data) > 0 else 0)\n",
    "    bars = ax.bar(x + i * width, vals, width, label=GROUP_LABELS[g], color=GROUP_COLORS[g])\n",
    "    for bar, val in zip(bars, vals):\n",
    "        if val > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{val:.2f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "ax.axhline(y=0.80, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Four-fifths threshold (0.80)')\n",
    "ax.set_xlabel('Experimental Arm')\n",
    "ax.set_ylabel('Selection Rate Ratio (vs. highest group)')\n",
    "ax.set_title('EEOC Four-Fifths Rule Check', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([ARM_LABELS[a] for a in arm_order])\n",
    "ax.set_ylim(0, 1.3)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig10_fourfifths.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print adverse impact flags\n",
    "print('\\nAdverse Impact Flags (ratio < 0.80):')\n",
    "for _, row in ff_df.iterrows():\n",
    "    if row['ratio'] < 0.80:\n",
    "        print(f\"  {ARM_LABELS[row['arm']]:15s} | {GROUP_LABELS[row['group']]:15s} | ratio = {row['ratio']:.3f} ** ADVERSE IMPACT **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binomial-header",
   "metadata": {},
   "source": [
    "## 11. Statistical Tests\n",
    "\n",
    "Binomial tests on pairwise race comparisons: is the selection rate significantly different from 50%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binomial-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Binomial Tests: Pairwise Race Comparisons')\n",
    "print('=' * 80)\n",
    "print(f'{\"Arm\":15s} | {\"Pair\":45s} | {\"White\":6s} | {\"Black\":6s} | {\"Total\":6s} | {\"p-value\":10s} | Sig?')\n",
    "print('-' * 80)\n",
    "\n",
    "race_pairs_test = [\n",
    "    ('white_male', 'black_male', 'Greg Walsh vs Darnell Jefferson'),\n",
    "    ('white_female', 'black_female', 'Emily Sullivan vs Lakisha Washington'),\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    for ga, gb, label in race_pairs_test:\n",
    "        pair_trials = arm_valid[\n",
    "            ((arm_valid['first_group'] == ga) & (arm_valid['second_group'] == gb)) |\n",
    "            ((arm_valid['first_group'] == gb) & (arm_valid['second_group'] == ga))\n",
    "        ]\n",
    "        if len(pair_trials) == 0:\n",
    "            print(f'{ARM_LABELS[arm]:15s} | {label:45s} | {\"--\":>6s} | {\"--\":>6s} | {\"0\":>6s} | {\"N/A\":>10s} | --')\n",
    "            continue\n",
    "        white_wins = (pair_trials['selected_group'] == ga).sum()\n",
    "        black_wins = (pair_trials['selected_group'] == gb).sum()\n",
    "        total = white_wins + black_wins\n",
    "        if total > 0:\n",
    "            result = stats.binomtest(white_wins, total, 0.5)\n",
    "            p = result.pvalue\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f'{ARM_LABELS[arm]:15s} | {label:45s} | {white_wins:>6d} | {black_wins:>6d} | {total:>6d} | {p:>10.4f} | {sig}')\n",
    "            test_results.append({'arm': arm, 'pair': label, 'white': white_wins,\n",
    "                                'black': black_wins, 'total': total, 'p': p})\n",
    "\n",
    "print('\\n* p<0.05, ** p<0.01, *** p<0.001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disparity-reduction-header",
   "metadata": {},
   "source": [
    "## 12. MCP Bias Reduction Summary\n",
    "\n",
    "Quantify how much the MCP pipeline reduces racial disparity compared to the raw baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disparity-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute racial disparity for each arm\n",
    "disparities = {}\n",
    "for arm in arm_order:\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    if len(arm_valid) == 0:\n",
    "        disparities[arm] = {'disparity': None, 'white': None, 'black': None, 'n': 0}\n",
    "        continue\n",
    "    rates = compute_race_rates(arm_valid)\n",
    "    w = rates.get('White', 0)\n",
    "    b = rates.get('Black', 0)\n",
    "    disparities[arm] = {'disparity': (w - b) * 100, 'white': w * 100, 'black': b * 100, 'n': len(arm_valid)}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "arms_with_data = [a for a in arm_order if disparities[a]['disparity'] is not None]\n",
    "disp_vals = [disparities[a]['disparity'] for a in arms_with_data]\n",
    "colors = [ARM_COLORS[a] for a in arms_with_data]\n",
    "\n",
    "bars = ax.bar([ARM_LABELS[a] for a in arms_with_data], disp_vals, color=colors, width=0.5)\n",
    "for bar, val, arm in zip(bars, disp_vals, arms_with_data):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5 if val >= 0 else bar.get_height() - 2,\n",
    "            f'{val:+.1f}pp', ha='center', va='bottom' if val >= 0 else 'top',\n",
    "            fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_ylabel('Racial Disparity\\n(White rate - Black rate, percentage points)')\n",
    "ax.set_title('Racial Disparity by Arm\\n(0 = no disparity, positive = pro-White bias)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(min(min(disp_vals) - 10, -10), max(max(disp_vals) + 10, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig11_disparity_reduction.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print('\\n=== BIAS REDUCTION SUMMARY ===')\n",
    "for arm in arm_order:\n",
    "    d = disparities[arm]\n",
    "    if d['disparity'] is not None:\n",
    "        print(f\"\\n{ARM_LABELS[arm]} (n={d['n']} valid trials):\")\n",
    "        print(f\"  White selection rate: {d['white']:.1f}%\")\n",
    "        print(f\"  Black selection rate: {d['black']:.1f}%\")\n",
    "        print(f\"  Disparity: {d['disparity']:+.1f} percentage points\")\n",
    "\n",
    "if disparities['raw_naive']['disparity'] is not None and disparities['mcp']['disparity'] is not None:\n",
    "    raw_disp = disparities['raw_naive']['disparity']\n",
    "    mcp_disp = disparities['mcp']['disparity']\n",
    "    reduction = raw_disp - mcp_disp\n",
    "    reduction_pct = (reduction / abs(raw_disp)) * 100 if raw_disp != 0 else 0\n",
    "    print(f\"\\n--- MCP vs Raw Naive ---\")\n",
    "    print(f\"  Disparity reduction: {reduction:.1f} percentage points\")\n",
    "    print(f\"  Relative reduction: {reduction_pct:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refusal-header",
   "metadata": {},
   "source": [
    "## 13. Refusal Rate Analysis\n",
    "\n",
    "Refusal rates by arm and criteria tier â€” the model's safety training interacts with demographic visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refusal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refusal rate by arm and criteria\n",
    "refusal_data = []\n",
    "for arm in arm_order:\n",
    "    arm_df_all = df[df['arm'] == arm]\n",
    "    for tier in criteria_tiers:\n",
    "        tier_df = arm_df_all[arm_df_all['criteria_tier'] == tier]\n",
    "        total = len(tier_df)\n",
    "        refused = (tier_df['selected_group'] == 'refused').sum()\n",
    "        unparseable = (tier_df['selected_group'] == 'unparseable').sum()\n",
    "        valid = total - refused - unparseable - (tier_df['selected_group'] == 'error').sum()\n",
    "        refusal_data.append({\n",
    "            'arm': arm, 'criteria': tier,\n",
    "            'refusal_rate': refused / total * 100 if total > 0 else 0,\n",
    "            'unparseable_rate': unparseable / total * 100 if total > 0 else 0,\n",
    "            'valid_rate': valid / total * 100 if total > 0 else 0,\n",
    "        })\n",
    "\n",
    "ref_df = pd.DataFrame(refusal_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "tier_colors = {'female_stereotyped': '#e74c3c', 'male_stereotyped': '#3498db', 'neutral': '#95a5a6'}\n",
    "tier_short = {'female_stereotyped': 'Nurturing', 'male_stereotyped': 'Tough', 'neutral': 'Neutral'}\n",
    "\n",
    "for arm_idx, arm in enumerate(arm_order):\n",
    "    arm_ref = ref_df[ref_df['arm'] == arm]\n",
    "    x = np.arange(len(criteria_tiers))\n",
    "\n",
    "    valid_vals = [arm_ref[arm_ref['criteria'] == t]['valid_rate'].values[0] for t in criteria_tiers]\n",
    "    refused_vals = [arm_ref[arm_ref['criteria'] == t]['refusal_rate'].values[0] for t in criteria_tiers]\n",
    "    unparse_vals = [arm_ref[arm_ref['criteria'] == t]['unparseable_rate'].values[0] for t in criteria_tiers]\n",
    "\n",
    "    width = 0.6\n",
    "    axes[arm_idx].bar(x, valid_vals, width, label='Valid', color='#2ecc71')\n",
    "    axes[arm_idx].bar(x, refused_vals, width, bottom=valid_vals, label='Refused', color='#e74c3c')\n",
    "    bottoms = [v + r for v, r in zip(valid_vals, refused_vals)]\n",
    "    axes[arm_idx].bar(x, unparse_vals, width, bottom=bottoms, label='Unparseable', color='#95a5a6')\n",
    "\n",
    "    axes[arm_idx].set_title(f'{ARM_LABELS[arm]}')\n",
    "    axes[arm_idx].set_xticks(x)\n",
    "    axes[arm_idx].set_xticklabels([tier_short[t] for t in criteria_tiers])\n",
    "    axes[arm_idx].set_ylim(0, 110)\n",
    "    if arm_idx == 0:\n",
    "        axes[arm_idx].set_ylabel('Percentage of Trials')\n",
    "    if arm_idx == 2:\n",
    "        axes[arm_idx].legend(fontsize=9)\n",
    "\n",
    "fig.suptitle('Response Outcome by Arm and Criteria Tier', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig12_refusal_by_criteria.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 14. Summary Table\n",
    "\n",
    "All key metrics in one table for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for arm in arm_order:\n",
    "    arm_all = df[df['arm'] == arm]\n",
    "    arm_valid = valid_df[valid_df['arm'] == arm]\n",
    "    total = len(arm_all)\n",
    "    n_valid = len(arm_valid)\n",
    "    n_refused = (arm_all['selected_group'] == 'refused').sum()\n",
    "    n_unparse = (arm_all['selected_group'] == 'unparseable').sum()\n",
    "\n",
    "    race_rates = compute_race_rates(arm_valid) if n_valid > 0 else {}\n",
    "    gender_rates = compute_gender_rates(arm_valid) if n_valid > 0 else {}\n",
    "\n",
    "    first = (arm_valid['selected_group'] == arm_valid['first_group']).sum() if n_valid > 0 else 0\n",
    "\n",
    "    rates = compute_selection_rates(arm_valid) if n_valid > 0 else {}\n",
    "    max_rate = max(rates.values()) if rates else 0\n",
    "    adverse_count = sum(1 for g in rates if rates[g] / max_rate < 0.80) if max_rate > 0 else 0\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Arm': ARM_LABELS[arm],\n",
    "        'Total Trials': total,\n",
    "        'Valid': n_valid,\n",
    "        'Refused': n_refused,\n",
    "        'Unparseable': n_unparse,\n",
    "        'Valid %': f'{n_valid/total*100:.0f}%' if total > 0 else 'N/A',\n",
    "        'White Rate': f'{race_rates.get(\"White\", 0)*100:.1f}%' if n_valid > 0 else 'N/A',\n",
    "        'Black Rate': f'{race_rates.get(\"Black\", 0)*100:.1f}%' if n_valid > 0 else 'N/A',\n",
    "        'Race Gap': f'{(race_rates.get(\"White\", 0) - race_rates.get(\"Black\", 0))*100:+.1f}pp' if n_valid > 0 else 'N/A',\n",
    "        'Male Rate': f'{gender_rates.get(\"Male\", 0)*100:.1f}%' if n_valid > 0 else 'N/A',\n",
    "        'Female Rate': f'{gender_rates.get(\"Female\", 0)*100:.1f}%' if n_valid > 0 else 'N/A',\n",
    "        '1st-Position %': f'{first/n_valid*100:.1f}%' if n_valid > 0 else 'N/A',\n",
    "        'Adverse Impact Groups': adverse_count,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.set_index('Arm', inplace=True)\n",
    "\n",
    "# Style the table\n",
    "display(summary_df.T)\n",
    "print('\\n(Race Gap = White selection rate - Black selection rate; 0 = parity)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
